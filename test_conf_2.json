{

   "task" : {
      // Deep Symbolic Regression
      "task_type" : "regression",

      // To customize a function set, edit this! See functions.py for a list of
      // supported functions. Note "const" will add placeholder constants that
      // will be optimized within the training loop. This will considerably
      // increase runtime.
      "function_set": ["add", "mul", "sub", "exp","log", "const"],
//      "function_set": ["add", "sub", "mul"],
//      "function_set": ["add", "sub", "mul", "const", "div", "exp", "log"],

      // Metric to be used for the reward function. See regression.py for
      // supported metrics.
      "metric" : "inv_nrmse",
      "metric_params" : [1.0],

      // Optional alternate metric to be used at evaluation time.
      "extra_metric_test" : null,
      "extra_metric_test_params" : [],

      // NRMSE threshold for early stopping. This is useful for noiseless
      // benchmark problems when DSO discovers the true solution.
      "threshold" : 1e-3,

      // With protected=false, floating-point errors (e.g. log of negative
      // number) will simply returns a minimal reward. With protected=true,
      // "protected" functions will prevent floating-point errors, but may
      // introduce discontinuities in the learned functions.
      "protected" : true,

      // You can add artificial reward noise directly to the reward function.
      // Note this does NOT add noise to the dataset.
      "reward_noise" : 0.0,
      "reward_noise_type" : "r",
      "normalize_variance" : false,

      // Set of thresholds (shared by all input variables) for building
      // decision trees. Note that no StateChecker will be added to Library
      // if decision_tree_threshold_set is an empty list or null.
      "decision_tree_threshold_set" : [],

      // Parameters for optimizing the "poly" token.
      // Note: poly_optimizer is turned on if and only if "poly" is in function_set.
      "poly_optimizer_params" : {
         // The (maximal) degree of the polynomials used to fit the data
         "degree": 5,
         // Cutoff value for the coefficients of polynomials. Coefficients
         // with magnitude less than this value will be regarded as 0.
         "coef_tol": 1e-6,
         // linear models from sklearn: linear_regression, lasso,
         // and ridge are currently supported, or our own implementation
         // of least squares regressor "dso_least_squares".
         "regressor": "dso_least_squares",
         "regressor_params": {
            // Cutoff value for p-value of coefficients. Coefficients with
            // larger p-values are forced to zero.
            "cutoff_p_value": 1.0,
            // Maximum number of terms in the polynomial. If more coefficients are nonzero,
            // coefficients with larger p-values will be forced to zero.
            "n_max_terms": null,
            // Cutoff value for the coefficients of polynomials. Coefficients
            // with magnitude less than this value will be regarded as 0.
            "coef_tol": 1e-6
         }
      }
   },

   // Experiment configuration.
   "experiment" : {

      // Random number seed. Don't forget to change this for multiple runs!
      "seed" : 0,
      "early_stopping": {
       "on": true,
       "metric": "inv_nrmse",
       "threshold": 1e-2,
       "patience": 10
   }
   },


   // Hyperparameters related to the main training loop.
   "training" : {

      // These parameters control the length of the run.
      "n_samples" : 10000, // 130000
      "batch_size" : 500,

      // To use the risk-seeking policy gradient, set epsilon < 1.0 and
      // baseline="R_e"
      "epsilon" : 0.05,
      "baseline" : "R_e",

      // Control variate parameters for vanilla policy gradient. If risk-seeking
      // is used, these have no effect.
      "alpha" : 0.5,
      "b_jumpstart" : false,

      // Number of cores to use when evaluating a batch of rewards. For batch
      // runs using run.py and --runs > 1, this will be overridden to 1. For
      // single runs, recommended to set this to as many cores as you can use!
      "n_cores_batch" : 1,

      // The complexity measure is only used to compute a Pareto front. It does
      // not affect the optimization.
      "complexity" : "token",

      // The constant optimizer used to optimized each "const" token.
      "const_optimizer" : "scipy",
      "const_params" : {
         "method" : "L-BFGS-B",
         "options" : {
            "gtol" : 1e-3
         }
      },

   },



   // Hyperparameters related to the policy, i.e. parameterized distribution over objects.
   "policy" : {

      // Type of policy
      // "rnn" is equivalent to "dso.policy.rnn_policy:RNNPolicy"
      "policy_type" : "rnn",

      // Maximum sequence length.
      "max_length" : 16,

      // Policy parameters
      "cell" : "lstm",
      "num_layers" : 1,
      "num_units" : 32,
      "initializer" : "zeros"
   },

   // Hyperparameters related to the discrete distribution model over objects.
   "policy_optimizer" : {

      // Type of policy optimizer
      // "pg" is equivalent to "dso.policy_optimizer.pg_policy_optimizer:PGPolicyOptimizer"
      "policy_optimizer_type" : "pg",

      // Whether to compute TensorBoard summaries.
      "summary" : false,

      // Optimizer hyperparameters.
      "optimizer" : "adam",

      // Entropy regularizer hyperparameters.
      "learning_rate" : 0.04,
      "entropy_weight" : 0.01, // 0.03
      "entropy_gamma" : 0.7
   },

   // Hyperparameters related to genetic programming hybrid methods.
   "gp_meld" : {
      "run_gp_meld" : false,
      "verbose" : false,
      "generations" : 20,
      "p_crossover" : 0.5,
      "p_mutate" : 0.5,
      "tournament_size" : 5,
      "train_n" : 50,
      "mutate_tree_max" : 3,
      // Speeds up processing when doing expensive evaluations.
      "parallel_eval" : false
   },

   // Hyperparameters related to including in situ priors and constraints. Each
   // prior must explicitly be turned "on" or it will not be used. See
   // config_common.json for descriptions of each prior.
   "prior": {
      "length" : {
         "min_" : 4,
         "max_" : 16,
         "on" : true
      },
      "repeat" : {
         "tokens" : "const",
         "min_" : null,
         "max_" : 4,
         "on" : true
      },

      "repeat_exp" : {
         "tokens" : "expneg",
         "min_" : null,
         "max_" : 1,
         "on" : true
      },

      "inverse" : {
         "on" : true
      },
      "trig" : {
         "on" : true
      },
      "const" : {
         "on" : true
      },
      "no_inputs" : {
         "on" : true
      },
      "uniform_arity" : {
         "on" : true
      },
      "soft_length" : {
         "loc" : 10,
         "scale" : 5,
         "on" : false
      },
      "domain_range" : {
         "on" : false
      }
   }


}
